{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3841fc3",
   "metadata": {},
   "source": [
    "# Notebook 1: Data Collection\n",
    "\n",
    "This notebook demonstrates how to collect QA datasets from various sources for the AI Response Evaluation System.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. Collect data from Alpaca dataset\n",
    "2. Generate responses using OpenAI API\n",
    "3. Load custom datasets\n",
    "4. Validate and preview data\n",
    "5. Save datasets for annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba343cd",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(Path.cwd().parent / '.env')\n",
    "\n",
    "from src.data_collection import DatasetCollector\n",
    "from src.config import RAW_DATA_DIR, TARGET_DATASET_SIZE\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"✓ Data directory: {RAW_DATA_DIR}\")\n",
    "print(f\"✓ Target dataset size: {TARGET_DATASET_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795c49b4",
   "metadata": {},
   "source": [
    "## Method 1: Collect from Alpaca Dataset\n",
    "\n",
    "The Alpaca dataset contains instruction-following examples. This is the easiest and fastest way to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c104101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize collector\n",
    "collector = DatasetCollector()\n",
    "\n",
    "# Collect 100 samples from Alpaca (adjust number as needed)\n",
    "print(\"Collecting data from Alpaca dataset...\")\n",
    "df_alpaca = collector.collect_from_alpaca(num_samples=100)\n",
    "\n",
    "print(f\"\\n✓ Collected {len(df_alpaca)} samples\")\n",
    "print(f\"\\nDataset shape: {df_alpaca.shape}\")\n",
    "print(f\"Columns: {list(df_alpaca.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "print(\"First 5 samples:\")\n",
    "df_alpaca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6424336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a specific example\n",
    "idx = 0\n",
    "print(f\"Question {idx+1}:\")\n",
    "print(df_alpaca.iloc[idx]['question'])\n",
    "print(f\"\\nResponse:\")\n",
    "print(df_alpaca.iloc[idx]['model_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6995f7e",
   "metadata": {},
   "source": [
    "## Method 2: Generate Responses Using OpenAI API\n",
    "\n",
    "Generate responses for custom questions using OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8cadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if API key is set\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if api_key:\n",
    "    print(\"✓ OpenAI API key found\")\n",
    "    print(f\"Key starts with: {api_key[:10]}...\")\n",
    "else:\n",
    "    print(\"⚠ OpenAI API key not found. Set OPENAI_API_KEY in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c7c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample questions\n",
    "sample_questions = collector.create_sample_questions(20)\n",
    "\n",
    "print(f\"Created {len(sample_questions)} sample questions\")\n",
    "print(\"\\nFirst 5 questions:\")\n",
    "for i, q in enumerate(sample_questions[:5], 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdd838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate responses using OpenAI (only if API key is available)\n",
    "if api_key:\n",
    "    print(\"Generating responses with OpenAI...\")\n",
    "    # Use only first 5 questions to save API calls\n",
    "    df_openai = collector.collect_from_openai(sample_questions[:5])\n",
    "    \n",
    "    print(f\"\\n✓ Generated {len(df_openai)} responses\")\n",
    "    display(df_openai.head())\n",
    "else:\n",
    "    print(\"⚠ Skipping OpenAI generation - API key not set\")\n",
    "    df_openai = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1345f5",
   "metadata": {},
   "source": [
    "## Method 3: Load Custom Dataset\n",
    "\n",
    "Load your own CSV file with question-response pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb4922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sample dataset that comes with the project\n",
    "sample_file = Path.cwd().parent / 'data' / 'raw' / 'sample_qa_dataset.csv'\n",
    "\n",
    "if sample_file.exists():\n",
    "    df_custom = collector.load_custom_dataset(sample_file)\n",
    "    print(f\"✓ Loaded {len(df_custom)} samples from {sample_file.name}\")\n",
    "    display(df_custom)\n",
    "else:\n",
    "    print(\"⚠ Sample file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c250410",
   "metadata": {},
   "source": [
    "## Data Validation and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24a6963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Alpaca data for analysis\n",
    "df = df_alpaca\n",
    "\n",
    "print(\"Dataset Statistics:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"\\nMissing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nQuestion length statistics:\")\n",
    "df['question_length'] = df['question'].str.len()\n",
    "print(df['question_length'].describe())\n",
    "\n",
    "print(f\"\\nResponse length statistics:\")\n",
    "df['response_length'] = df['model_response'].str.len()\n",
    "print(df['response_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90952218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize length distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Question lengths\n",
    "axes[0].hist(df['question_length'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].set_xlabel('Question Length (characters)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Distribution of Question Lengths')\n",
    "axes[0].axvline(df['question_length'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0].legend()\n",
    "\n",
    "# Response lengths\n",
    "axes[1].hist(df['response_length'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
    "axes[1].set_xlabel('Response Length (characters)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Distribution of Response Lengths')\n",
    "axes[1].axvline(df['response_length'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d8e63",
   "metadata": {},
   "source": [
    "## Save Dataset for Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a923a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the collected dataset\n",
    "output_filename = \"my_qa_dataset.csv\"\n",
    "\n",
    "# Remove temporary columns\n",
    "df_to_save = df[['question', 'model_response']].copy()\n",
    "\n",
    "collector.save_dataset(df_to_save, output_filename)\n",
    "\n",
    "print(f\"\\n✓ Dataset saved successfully!\")\n",
    "print(f\"Location: {RAW_DATA_DIR / output_filename}\")\n",
    "print(f\"\\nNext step: Annotate this dataset using notebook 02_annotation.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fc3232",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, you learned how to:\n",
    "- ✓ Collect data from Alpaca dataset (fast and free)\n",
    "- ✓ Generate responses using OpenAI API (requires API key)\n",
    "- ✓ Load custom CSV datasets\n",
    "- ✓ Validate and analyze the data\n",
    "- ✓ Save datasets for annotation\n",
    "\n",
    "### Next Steps:\n",
    "1. Open `02_annotation.ipynb` to annotate your dataset\n",
    "2. Or run: `python main.py --annotate --dataset \"data/raw/my_qa_dataset.csv\"`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
